{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce62619",
   "metadata": {},
   "source": [
    "# AI Fitness Mirror - Pose-Based Activity Recognition\n",
    "\n",
    "This notebook implements a complete pipeline for:\n",
    "1. **Pose Estimation**: Extract skeletal keypoints from images/videos using MediaPipe\n",
    "2. **Activity Classification**: Train a lightweight DNN to classify activities (pushups, situps, etc.)\n",
    "3. **Pose Correctness Assessment**: Evaluate form quality using learned patterns from correct examples\n",
    "4. **Real-time Testing**: Test on images, videos, and live webcam feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7d97e",
   "metadata": {},
   "source": [
    "## 0. Install Required Libraries\n",
    "\n",
    "Run this cell first to install all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python mediapipe numpy matplotlib tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7fbbd7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfff6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"MediaPipe available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d1902",
   "metadata": {},
   "source": [
    "## 2. Configure Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4012df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Configuration\n",
    "BASE_DIR = Path(r\"c:\\Users\\Jiesheng He\\OneDrive\\Documents\\aifitnessmirror\")\n",
    "TRAINING_DATA_DIR = BASE_DIR / \"training_data\"  # Should contain subfolders: pushups, situps, etc.\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "VISUALIZATION_DIR = OUTPUT_DIR / \"visualizations\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "JSON_OUTPUT_PATH = OUTPUT_DIR / \"all_keypoints.json\"\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "VISUALIZATION_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# MediaPipe Pose Configuration\n",
    "POSE_CONFIDENCE = 0.5  # Minimum detection confidence\n",
    "TRACKING_CONFIDENCE = 0.5  # Minimum tracking confidence\n",
    "\n",
    "# Model Configuration\n",
    "SEQUENCE_LENGTH = 30  # Number of frames to consider for video sequences\n",
    "NUM_KEYPOINTS = 33  # MediaPipe provides 33 landmarks\n",
    "KEYPOINT_FEATURES = 4  # x, y, z, visibility per keypoint\n",
    "INPUT_SHAPE = NUM_KEYPOINTS * KEYPOINT_FEATURES  # Flattened keypoints\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "print(f\"Configuration complete!\")\n",
    "print(f\"Training data directory: {TRAINING_DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Visualizations will be saved to: {VISUALIZATION_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c589e70",
   "metadata": {},
   "source": [
    "## 3. Load and Process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load images and videos from class-labeled folders\n",
    "    Expected structure: training_data/pushups/, training_data/situps/, etc.\n",
    "    Returns: dictionary with activity labels and file paths\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'images': {},\n",
    "        'videos': {},\n",
    "        'classes': []\n",
    "    }\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        print(f\"Warning: Training data directory not found: {data_dir}\")\n",
    "        print(f\"Please create the directory and add subfolders for each activity class.\")\n",
    "        print(f\"Example structure:\")\n",
    "        print(f\"  {data_dir}/\")\n",
    "        print(f\"    pushups/\")\n",
    "        print(f\"      image1.jpg, video1.mp4, ...\")\n",
    "        print(f\"    situps/\")\n",
    "        print(f\"      image1.jpg, video1.mp4, ...\")\n",
    "        return data\n",
    "    \n",
    "    # Supported file extensions\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    video_extensions = {'.mp4', '.avi', '.mov', '.mkv'}\n",
    "    \n",
    "    # Scan each class folder\n",
    "    for class_folder in data_dir.iterdir():\n",
    "        if class_folder.is_dir():\n",
    "            class_name = class_folder.name\n",
    "            data['classes'].append(class_name)\n",
    "            data['images'][class_name] = []\n",
    "            data['videos'][class_name] = []\n",
    "            \n",
    "            # Scan files in class folder\n",
    "            for file_path in class_folder.iterdir():\n",
    "                if file_path.suffix.lower() in image_extensions:\n",
    "                    data['images'][class_name].append(file_path)\n",
    "                elif file_path.suffix.lower() in video_extensions:\n",
    "                    data['videos'][class_name].append(file_path)\n",
    "            \n",
    "            print(f\"Class '{class_name}': {len(data['images'][class_name])} images, {len(data['videos'][class_name])} videos\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load training data\n",
    "training_data = load_training_data(TRAINING_DATA_DIR)\n",
    "print(f\"\\nTotal classes found: {len(training_data['classes'])}\")\n",
    "print(f\"Classes: {training_data['classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214de562",
   "metadata": {},
   "source": [
    "## 4. Extract Pose Keypoints from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac8c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_image(image_path, pose_detector):\n",
    "    \"\"\"\n",
    "    Extract pose keypoints from a single image using MediaPipe\n",
    "    Returns: keypoints array (33 landmarks x 4 features) or None if no pose detected\n",
    "    \"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose_detector.process(image_rgb)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        # Extract keypoints: x, y, z, visibility for each of 33 landmarks\n",
    "        keypoints = []\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "        return np.array(keypoints), image\n",
    "    \n",
    "    return None, image\n",
    "\n",
    "def process_images(image_dict, class_name):\n",
    "    \"\"\"\n",
    "    Process all images for a given class and extract keypoints\n",
    "    \"\"\"\n",
    "    keypoints_data = []\n",
    "    \n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=True,\n",
    "        min_detection_confidence=POSE_CONFIDENCE,\n",
    "        min_tracking_confidence=TRACKING_CONFIDENCE\n",
    "    ) as pose:\n",
    "        \n",
    "        for img_path in image_dict[class_name]:\n",
    "            keypoints, image = extract_keypoints_from_image(img_path, pose)\n",
    "            \n",
    "            if keypoints is not None:\n",
    "                keypoints_data.append({\n",
    "                    'file': str(img_path),\n",
    "                    'class': class_name,\n",
    "                    'type': 'image',\n",
    "                    'keypoints': keypoints.tolist(),\n",
    "                    'image': image\n",
    "                })\n",
    "    \n",
    "    print(f\"Extracted keypoints from {len(keypoints_data)}/{len(image_dict[class_name])} images for class '{class_name}'\")\n",
    "    return keypoints_data\n",
    "\n",
    "# Process all image classes\n",
    "all_image_keypoints = []\n",
    "for class_name in training_data['classes']:\n",
    "    if class_name in training_data['images'] and len(training_data['images'][class_name]) > 0:\n",
    "        class_keypoints = process_images(training_data['images'], class_name)\n",
    "        all_image_keypoints.extend(class_keypoints)\n",
    "\n",
    "print(f\"\\nTotal images processed: {len(all_image_keypoints)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7da4ae",
   "metadata": {},
   "source": [
    "## 5. Extract Pose Keypoints from Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_video(video_path, pose_detector, sample_rate=5):\n",
    "    \"\"\"\n",
    "    Extract pose keypoints from video frames\n",
    "    sample_rate: process every Nth frame to reduce computation\n",
    "    Returns: list of keypoint arrays and corresponding frames\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to load video: {video_path}\")\n",
    "        return [], []\n",
    "    \n",
    "    keypoints_sequence = []\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Sample frames to reduce processing time\n",
    "        if frame_count % sample_rate == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose_detector.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                keypoints = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "                keypoints_sequence.append(np.array(keypoints))\n",
    "                frames.append(frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return keypoints_sequence, frames\n",
    "\n",
    "def process_videos(video_dict, class_name):\n",
    "    \"\"\"\n",
    "    Process all videos for a given class and extract keypoint sequences\n",
    "    \"\"\"\n",
    "    video_keypoints_data = []\n",
    "    \n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        min_detection_confidence=POSE_CONFIDENCE,\n",
    "        min_tracking_confidence=TRACKING_CONFIDENCE\n",
    "    ) as pose:\n",
    "        \n",
    "        for vid_path in video_dict[class_name]:\n",
    "            keypoints_seq, frames = extract_keypoints_from_video(vid_path, pose)\n",
    "            \n",
    "            if len(keypoints_seq) > 0:\n",
    "                video_keypoints_data.append({\n",
    "                    'file': str(vid_path),\n",
    "                    'class': class_name,\n",
    "                    'type': 'video',\n",
    "                    'keypoints_sequence': [kp.tolist() for kp in keypoints_seq],\n",
    "                    'frames': frames,\n",
    "                    'num_frames': len(keypoints_seq)\n",
    "                })\n",
    "    \n",
    "    print(f\"Extracted keypoints from {len(video_keypoints_data)}/{len(video_dict[class_name])} videos for class '{class_name}'\")\n",
    "    return video_keypoints_data\n",
    "\n",
    "# Process all video classes\n",
    "all_video_keypoints = []\n",
    "for class_name in training_data['classes']:\n",
    "    if class_name in training_data['videos'] and len(training_data['videos'][class_name]) > 0:\n",
    "        class_keypoints = process_videos(training_data['videos'], class_name)\n",
    "        all_video_keypoints.extend(class_keypoints)\n",
    "\n",
    "print(f\"\\nTotal videos processed: {len(all_video_keypoints)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cc131",
   "metadata": {},
   "source": [
    "## 6. Visualize Skeletal Models on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pose_on_image(image, keypoints):\n",
    "    \"\"\"\n",
    "    Draw skeletal pose overlay on image using MediaPipe landmarks\n",
    "    \"\"\"\n",
    "    annotated_image = image.copy()\n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    # Convert flat keypoints back to landmark format for visualization\n",
    "    landmarks = []\n",
    "    for i in range(0, len(keypoints), 4):\n",
    "        x, y, z, vis = keypoints[i:i+4]\n",
    "        landmarks.append((int(x * w), int(y * h), vis))\n",
    "    \n",
    "    # Define MediaPipe pose connections\n",
    "    connections = mp_pose.POSE_CONNECTIONS\n",
    "    \n",
    "    # Draw connections\n",
    "    for connection in connections:\n",
    "        start_idx, end_idx = connection\n",
    "        if start_idx < len(landmarks) and end_idx < len(landmarks):\n",
    "            start_point = landmarks[start_idx][:2]\n",
    "            end_point = landmarks[end_idx][:2]\n",
    "            if landmarks[start_idx][2] > 0.5 and landmarks[end_idx][2] > 0.5:  # Check visibility\n",
    "                cv2.line(annotated_image, start_point, end_point, (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw keypoints\n",
    "    for i, (x, y, vis) in enumerate(landmarks):\n",
    "        if vis > 0.5:\n",
    "            cv2.circle(annotated_image, (x, y), 5, (0, 0, 255), -1)\n",
    "    \n",
    "    return annotated_image\n",
    "\n",
    "def save_image_visualizations(image_keypoints_data):\n",
    "    \"\"\"\n",
    "    Save skeletal overlays for all processed images\n",
    "    \"\"\"\n",
    "    img_viz_dir = VISUALIZATION_DIR / \"images\"\n",
    "    img_viz_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for i, data in enumerate(image_keypoints_data):\n",
    "        annotated_image = draw_pose_on_image(data['image'], data['keypoints'])\n",
    "        \n",
    "        # Create output filename\n",
    "        original_name = Path(data['file']).stem\n",
    "        output_name = f\"{data['class']}_{original_name}_pose.jpg\"\n",
    "        output_path = img_viz_dir / output_name\n",
    "        \n",
    "        cv2.imwrite(str(output_path), annotated_image)\n",
    "    \n",
    "    print(f\"Saved {len(image_keypoints_data)} image visualizations to {img_viz_dir}\")\n",
    "\n",
    "# Generate and save image visualizations\n",
    "if len(all_image_keypoints) > 0:\n",
    "    save_image_visualizations(all_image_keypoints)\n",
    "    print(\"Image visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77d54c",
   "metadata": {},
   "source": [
    "## 7. Visualize Skeletal Models on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d5ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video_visualizations(video_keypoints_data):\n",
    "    \"\"\"\n",
    "    Create and save videos with skeletal overlays\n",
    "    \"\"\"\n",
    "    vid_viz_dir = VISUALIZATION_DIR / \"videos\"\n",
    "    vid_viz_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for data in video_keypoints_data:\n",
    "        if len(data['frames']) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get video properties\n",
    "        sample_frame = data['frames'][0]\n",
    "        h, w, _ = sample_frame.shape\n",
    "        \n",
    "        # Create output filename\n",
    "        original_name = Path(data['file']).stem\n",
    "        output_name = f\"{data['class']}_{original_name}_pose.mp4\"\n",
    "        output_path = vid_viz_dir / output_name\n",
    "        \n",
    "        # Create video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        fps = 6  # Reduced FPS since we sampled frames\n",
    "        out = cv2.VideoWriter(str(output_path), fourcc, fps, (w, h))\n",
    "        \n",
    "        # Process each frame\n",
    "        for frame, keypoints in zip(data['frames'], data['keypoints_sequence']):\n",
    "            annotated_frame = draw_pose_on_image(frame, keypoints)\n",
    "            out.write(annotated_frame)\n",
    "        \n",
    "        out.release()\n",
    "    \n",
    "    print(f\"Saved {len(video_keypoints_data)} video visualizations to {vid_viz_dir}\")\n",
    "\n",
    "# Generate and save video visualizations\n",
    "if len(all_video_keypoints) > 0:\n",
    "    save_video_visualizations(all_video_keypoints)\n",
    "    print(\"Video visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec082175",
   "metadata": {},
   "source": [
    "## 8. Save Keypoints to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keypoints_to_json(image_data, video_data, output_path):\n",
    "    \"\"\"\n",
    "    Compile all keypoints into a single JSON file following COCO-like format\n",
    "    \"\"\"\n",
    "    json_data = {\n",
    "        'info': {\n",
    "            'description': 'AI Fitness Mirror - Pose Keypoints Dataset',\n",
    "            'version': '1.0',\n",
    "            'date_created': datetime.now().isoformat(),\n",
    "            'num_keypoints': NUM_KEYPOINTS,\n",
    "            'keypoint_format': 'MediaPipe Pose (33 landmarks with x, y, z, visibility)'\n",
    "        },\n",
    "        'classes': training_data['classes'],\n",
    "        'images': [],\n",
    "        'videos': []\n",
    "    }\n",
    "    \n",
    "    # Add image data\n",
    "    for i, data in enumerate(image_data):\n",
    "        json_data['images'].append({\n",
    "            'id': i,\n",
    "            'file_path': data['file'],\n",
    "            'class': data['class'],\n",
    "            'keypoints': data['keypoints']\n",
    "        })\n",
    "    \n",
    "    # Add video data\n",
    "    for i, data in enumerate(video_data):\n",
    "        # Remove frames from JSON (too large), keep only keypoints\n",
    "        json_data['videos'].append({\n",
    "            'id': i,\n",
    "            'file_path': data['file'],\n",
    "            'class': data['class'],\n",
    "            'num_frames': data['num_frames'],\n",
    "            'keypoints_sequence': data['keypoints_sequence']\n",
    "        })\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved keypoints data to {output_path}\")\n",
    "    print(f\"  - Images: {len(json_data['images'])}\")\n",
    "    print(f\"  - Videos: {len(json_data['videos'])}\")\n",
    "    print(f\"  - Total classes: {len(json_data['classes'])}\")\n",
    "\n",
    "# Save all keypoints to JSON\n",
    "save_keypoints_to_json(all_image_keypoints, all_video_keypoints, JSON_OUTPUT_PATH)\n",
    "print(\"\\nJSON file ready for manual verification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c087435",
   "metadata": {},
   "source": [
    "## 9. Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(image_data, video_data):\n",
    "    \"\"\"\n",
    "    Convert keypoints to normalized feature vectors for training\n",
    "    \"\"\"\n",
    "    X = []  # Features (keypoints)\n",
    "    y = []  # Labels (activity classes)\n",
    "    \n",
    "    # Process images\n",
    "    for data in image_data:\n",
    "        X.append(data['keypoints'])\n",
    "        y.append(data['class'])\n",
    "    \n",
    "    # Process videos - extract individual frames\n",
    "    for data in video_data:\n",
    "        for keypoints in data['keypoints_sequence']:\n",
    "            X.append(keypoints)\n",
    "            y.append(data['class'])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    \n",
    "    print(f\"Dataset prepared:\")\n",
    "    print(f\"  - Total samples: {len(X)}\")\n",
    "    print(f\"  - Feature shape: {X.shape}\")\n",
    "    print(f\"  - Classes: {label_encoder.classes_}\")\n",
    "    print(f\"  - Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "    \n",
    "    return X, y_categorical, label_encoder\n",
    "\n",
    "# Prepare data\n",
    "if len(all_image_keypoints) > 0 or len(all_video_keypoints) > 0:\n",
    "    X_data, y_data, label_encoder = prepare_training_data(all_image_keypoints, all_video_keypoints)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_data, y_data, test_size=0.2, random_state=42, stratify=np.argmax(y_data, axis=1)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    \n",
    "    # Store for later use\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "else:\n",
    "    print(\"No training data available. Please add images/videos to the training_data folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd9755",
   "metadata": {},
   "source": [
    "## 10. Build Lightweight DNN Model for Activity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_activity_classifier(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build a lightweight DNN for activity classification from pose keypoints\n",
    "    Uses a simple feedforward network - very efficient for inference\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        \n",
    "        # Dense layers with dropout for regularization\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "if 'num_classes' in locals():\n",
    "    activity_model = build_activity_classifier(INPUT_SHAPE, num_classes)\n",
    "    activity_model.summary()\n",
    "    \n",
    "    print(f\"\\nActivity classification model created!\")\n",
    "    print(f\"Input: {INPUT_SHAPE} features (33 landmarks × 4 values)\")\n",
    "    print(f\"Output: {num_classes} activity classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d0158",
   "metadata": {},
   "source": [
    "## 11. Build Pose Correctness Assessment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8bafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_correctness_autoencoder(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build an autoencoder-based correctness model for each activity class\n",
    "    The model learns to reconstruct correct poses. Reconstruction error indicates incorrectness.\n",
    "    Low error = correct form, High error = incorrect form\n",
    "    \n",
    "    We create a shared encoder/decoder with class-specific branches\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = layers.Input(shape=(input_shape,))\n",
    "    class_input = layers.Input(shape=(num_classes,))  # One-hot encoded class\n",
    "    \n",
    "    # Concatenate keypoints with class information\n",
    "    combined = layers.Concatenate()([input_layer, class_input])\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = layers.Dense(128, activation='relu')(combined)\n",
    "    encoded = layers.Dropout(0.2)(encoded)\n",
    "    encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "    encoded = layers.Dropout(0.2)(encoded)\n",
    "    encoded = layers.Dense(32, activation='relu')(encoded)  # Bottleneck\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "    decoded = layers.Dropout(0.2)(decoded)\n",
    "    decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "    decoded = layers.Dropout(0.2)(decoded)\n",
    "    \n",
    "    # Reconstruct only the keypoints (not the class)\n",
    "    output_layer = layers.Dense(input_shape, activation='linear')(decoded)\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Model(inputs=[input_layer, class_input], outputs=output_layer)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',  # Mean squared error for reconstruction\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build correctness model\n",
    "if 'num_classes' in locals():\n",
    "    correctness_model = build_correctness_autoencoder(INPUT_SHAPE, num_classes)\n",
    "    correctness_model.summary()\n",
    "    \n",
    "    print(f\"\\nCorrectness assessment model created!\")\n",
    "    print(f\"This model learns the patterns of CORRECT poses.\")\n",
    "    print(f\"Higher reconstruction error = less correct form\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e2a73",
   "metadata": {},
   "source": [
    "## 12. Train Activity Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca89c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the activity classification model\n",
    "if 'activity_model' in locals() and 'X_train' in locals():\n",
    "    print(\"Training activity classification model...\")\n",
    "    \n",
    "    # Callbacks for better training\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = activity_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss, train_acc = activity_model.evaluate(X_train, y_train, verbose=0)\n",
    "    val_loss, val_acc = activity_model.evaluate(X_val, y_val, verbose=0)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Activity Classification Model Results:\")\n",
    "    print(f\"  Training Accuracy: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "    plt.title('Activity Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.title('Activity Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping training - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fa018",
   "metadata": {},
   "source": [
    "## 13. Train Pose Correctness Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the correctness assessment model\n",
    "if 'correctness_model' in locals() and 'X_train' in locals():\n",
    "    print(\"Training pose correctness model...\")\n",
    "    print(\"This model learns to reconstruct CORRECT poses (from training data)\")\n",
    "    print(\"Reconstruction error will be used to assess form quality\\n\")\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # Train to reconstruct the input keypoints\n",
    "    # The target is the same as input (autoencoder)\n",
    "    history_correctness = correctness_model.fit(\n",
    "        [X_train, y_train], X_train,  # Input: keypoints + class, Output: keypoints\n",
    "        validation_data=([X_val, y_val], X_val),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_loss = correctness_model.evaluate([X_train, y_train], X_train, verbose=0)\n",
    "    val_loss = correctness_model.evaluate([X_val, y_val], X_val, verbose=0)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Correctness Assessment Model Results:\")\n",
    "    print(f\"  Training MSE: {train_loss[0]:.6f}\")\n",
    "    print(f\"  Validation MSE: {val_loss[0]:.6f}\")\n",
    "    print(f\"  (Lower MSE = better reconstruction of correct poses)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Calculate baseline reconstruction errors for each class\n",
    "    print(\"\\nCalculating baseline reconstruction errors per class...\")\n",
    "    class_reconstruction_errors = {}\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        class_name = label_encoder.classes_[class_idx]\n",
    "        # Get samples for this class\n",
    "        class_mask = np.argmax(y_val, axis=1) == class_idx\n",
    "        if np.sum(class_mask) > 0:\n",
    "            X_class = X_val[class_mask]\n",
    "            y_class = y_val[class_mask]\n",
    "            \n",
    "            # Predict and calculate MSE\n",
    "            predictions = correctness_model.predict([X_class, y_class], verbose=0)\n",
    "            mse_per_sample = np.mean((X_class - predictions) ** 2, axis=1)\n",
    "            \n",
    "            # Store mean and std for this class\n",
    "            class_reconstruction_errors[class_name] = {\n",
    "                'mean': np.mean(mse_per_sample),\n",
    "                'std': np.std(mse_per_sample),\n",
    "                'threshold': np.mean(mse_per_sample) + 2 * np.std(mse_per_sample)  # 2-sigma threshold\n",
    "            }\n",
    "            \n",
    "            print(f\"  {class_name}: mean={class_reconstruction_errors[class_name]['mean']:.6f}, \"\n",
    "                  f\"threshold={class_reconstruction_errors[class_name]['threshold']:.6f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_correctness.history['loss'], label='Train')\n",
    "    plt.plot(history_correctness.history['val_loss'], label='Validation')\n",
    "    plt.title('Correctness Model Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_correctness.history['mae'], label='Train MAE')\n",
    "    plt.plot(history_correctness.history['val_mae'], label='Val MAE')\n",
    "    plt.title('Correctness Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping training - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5249f25",
   "metadata": {},
   "source": [
    "## 14. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and metadata\n",
    "if 'activity_model' in locals():\n",
    "    # Save activity classification model\n",
    "    activity_model_path = MODELS_DIR / \"activity_classifier.h5\"\n",
    "    activity_model.save(activity_model_path)\n",
    "    print(f\"Activity classifier saved to: {activity_model_path}\")\n",
    "    \n",
    "    # Save correctness model\n",
    "    correctness_model_path = MODELS_DIR / \"correctness_model.h5\"\n",
    "    correctness_model.save(correctness_model_path)\n",
    "    print(f\"Correctness model saved to: {correctness_model_path}\")\n",
    "    \n",
    "    # Save label encoder and class info\n",
    "    model_metadata = {\n",
    "        'classes': label_encoder.classes_.tolist(),\n",
    "        'num_classes': num_classes,\n",
    "        'input_shape': INPUT_SHAPE,\n",
    "        'num_keypoints': NUM_KEYPOINTS,\n",
    "        'class_reconstruction_errors': class_reconstruction_errors if 'class_reconstruction_errors' in locals() else {}\n",
    "    }\n",
    "    \n",
    "    metadata_path = MODELS_DIR / \"model_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2)\n",
    "    print(f\"Model metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    # Try to save as TensorFlow Lite for mobile deployment (optional)\n",
    "    try:\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(activity_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        tflite_path = MODELS_DIR / \"activity_classifier.tflite\"\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(f\"TensorFlow Lite model saved to: {tflite_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not convert to TFLite: {e}\")\n",
    "    \n",
    "    print(\"\\n✓ All models saved successfully!\")\n",
    "else:\n",
    "    print(\"No models to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c85895",
   "metadata": {},
   "source": [
    "## 15. Test on Single Image\n",
    "\n",
    "Test the trained models on a new image to classify the activity and assess pose correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_image(image_path, activity_model, correctness_model, label_encoder, class_errors):\n",
    "    \"\"\"\n",
    "    Test trained models on a single image\n",
    "    \"\"\"\n",
    "    # Extract keypoints\n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=True,\n",
    "        min_detection_confidence=POSE_CONFIDENCE\n",
    "    ) as pose:\n",
    "        keypoints, image = extract_keypoints_from_image(image_path, pose)\n",
    "    \n",
    "    if keypoints is None:\n",
    "        print(\"No pose detected in image!\")\n",
    "        return\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    keypoints_input = keypoints.reshape(1, -1)\n",
    "    \n",
    "    # Predict activity\n",
    "    activity_probs = activity_model.predict(keypoints_input, verbose=0)[0]\n",
    "    predicted_class_idx = np.argmax(activity_probs)\n",
    "    predicted_class = label_encoder.classes_[predicted_class_idx]\n",
    "    confidence = activity_probs[predicted_class_idx]\n",
    "    \n",
    "    # Assess correctness\n",
    "    class_one_hot = to_categorical([predicted_class_idx], num_classes=len(label_encoder.classes_))\n",
    "    reconstructed = correctness_model.predict([keypoints_input, class_one_hot], verbose=0)\n",
    "    reconstruction_error = np.mean((keypoints_input - reconstructed) ** 2)\n",
    "    \n",
    "    # Calculate correctness score\n",
    "    if predicted_class in class_errors:\n",
    "        threshold = class_errors[predicted_class]['threshold']\n",
    "        correctness_score = max(0, min(100, 100 * (1 - reconstruction_error / threshold)))\n",
    "    else:\n",
    "        correctness_score = 50  # Default if no baseline\n",
    "    \n",
    "    # Visualize\n",
    "    annotated_image = draw_pose_on_image(image, keypoints)\n",
    "    \n",
    "    # Add text annotations\n",
    "    h, w, _ = annotated_image.shape\n",
    "    cv2.putText(annotated_image, f\"Activity: {predicted_class}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated_image, f\"Confidence: {confidence*100:.1f}%\", (10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated_image, f\"Form Quality: {correctness_score:.1f}%\", (10, 110),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predicted: {predicted_class} | Confidence: {confidence*100:.1f}% | Form: {correctness_score:.1f}%\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Activity: {predicted_class}\")\n",
    "    print(f\"  Confidence: {confidence*100:.1f}%\")\n",
    "    print(f\"  Form Quality Score: {correctness_score:.1f}%\")\n",
    "    print(f\"  Reconstruction Error: {reconstruction_error:.6f}\")\n",
    "\n",
    "# Example usage (replace with your test image path)\n",
    "TEST_IMAGE_PATH = BASE_DIR / \"test_image.jpg\"\n",
    "\n",
    "if 'activity_model' in locals() and TEST_IMAGE_PATH.exists():\n",
    "    print(f\"Testing on image: {TEST_IMAGE_PATH}\")\n",
    "    test_on_image(TEST_IMAGE_PATH, activity_model, correctness_model, \n",
    "                  label_encoder, class_reconstruction_errors)\n",
    "else:\n",
    "    print(f\"To test on an image, place a test image at: {TEST_IMAGE_PATH}\")\n",
    "    print(\"Or modify TEST_IMAGE_PATH variable to point to your test image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aeef75",
   "metadata": {},
   "source": [
    "## 16. Test on Single Video\n",
    "\n",
    "Process a test video and generate annotated output with activity classification and form assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_video(video_path, activity_model, correctness_model, label_encoder, class_errors, output_path=None):\n",
    "    \"\"\"\n",
    "    Test trained models on a video file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to open video: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Create output video writer if path provided\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    predictions_log = []\n",
    "    \n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        min_detection_confidence=POSE_CONFIDENCE\n",
    "    ) as pose:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Extract keypoints\n",
    "                keypoints = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "                keypoints = np.array(keypoints)\n",
    "                \n",
    "                # Predict activity\n",
    "                keypoints_input = keypoints.reshape(1, -1)\n",
    "                activity_probs = activity_model.predict(keypoints_input, verbose=0)[0]\n",
    "                predicted_class_idx = np.argmax(activity_probs)\n",
    "                predicted_class = label_encoder.classes_[predicted_class_idx]\n",
    "                confidence = activity_probs[predicted_class_idx]\n",
    "                \n",
    "                # Assess correctness\n",
    "                class_one_hot = to_categorical([predicted_class_idx], num_classes=len(label_encoder.classes_))\n",
    "                reconstructed = correctness_model.predict([keypoints_input, class_one_hot], verbose=0)\n",
    "                reconstruction_error = np.mean((keypoints_input - reconstructed) ** 2)\n",
    "                \n",
    "                if predicted_class in class_errors:\n",
    "                    threshold = class_errors[predicted_class]['threshold']\n",
    "                    correctness_score = max(0, min(100, 100 * (1 - reconstruction_error / threshold)))\n",
    "                else:\n",
    "                    correctness_score = 50\n",
    "                \n",
    "                # Draw pose and annotations\n",
    "                annotated_frame = draw_pose_on_image(frame, keypoints)\n",
    "                cv2.putText(annotated_frame, f\"Activity: {predicted_class}\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_frame, f\"Confidence: {confidence*100:.1f}%\", (10, 60),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_frame, f\"Form: {correctness_score:.1f}%\", (10, 90),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Log prediction\n",
    "                predictions_log.append({\n",
    "                    'frame': frame_count,\n",
    "                    'activity': predicted_class,\n",
    "                    'confidence': float(confidence),\n",
    "                    'form_score': float(correctness_score)\n",
    "                })\n",
    "                \n",
    "                if output_path:\n",
    "                    out.write(annotated_frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {frame_count} frames...\")\n",
    "    \n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "        print(f\"Output video saved to: {output_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    if predictions_log:\n",
    "        df_predictions = pd.DataFrame(predictions_log) if 'pd' in dir() else None\n",
    "        print(f\"\\nVideo Analysis Summary:\")\n",
    "        print(f\"  Total frames processed: {frame_count}\")\n",
    "        print(f\"  Frames with pose detected: {len(predictions_log)}\")\n",
    "        \n",
    "        # Most common activity\n",
    "        activities = [p['activity'] for p in predictions_log]\n",
    "        most_common = max(set(activities), key=activities.count)\n",
    "        print(f\"  Most common activity: {most_common}\")\n",
    "        \n",
    "        # Average form score\n",
    "        avg_form = np.mean([p['form_score'] for p in predictions_log])\n",
    "        print(f\"  Average form quality: {avg_form:.1f}%\")\n",
    "\n",
    "# Example usage\n",
    "TEST_VIDEO_PATH = BASE_DIR / \"test_video.mp4\"\n",
    "OUTPUT_VIDEO_PATH = OUTPUT_DIR / \"test_video_annotated.mp4\"\n",
    "\n",
    "if 'activity_model' in locals() and TEST_VIDEO_PATH.exists():\n",
    "    print(f\"Testing on video: {TEST_VIDEO_PATH}\")\n",
    "    test_on_video(TEST_VIDEO_PATH, activity_model, correctness_model, \n",
    "                  label_encoder, class_reconstruction_errors, OUTPUT_VIDEO_PATH)\n",
    "else:\n",
    "    print(f\"To test on a video, place a test video at: {TEST_VIDEO_PATH}\")\n",
    "    print(\"Or modify TEST_VIDEO_PATH variable to point to your test video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da57923",
   "metadata": {},
   "source": [
    "## 17. Test on Live Video Feed\n",
    "\n",
    "Real-time activity recognition and form assessment using webcam feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_live_feed(activity_model, correctness_model, label_encoder, class_errors):\n",
    "    \"\"\"\n",
    "    Real-time activity recognition and form assessment using webcam\n",
    "    Press 'q' to quit\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    print(\"Starting live feed...\")\n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        min_detection_confidence=POSE_CONFIDENCE,\n",
    "        min_tracking_confidence=TRACKING_CONFIDENCE\n",
    "    ) as pose:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Flip frame for mirror effect\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Extract keypoints\n",
    "                keypoints = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    keypoints.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "                keypoints = np.array(keypoints)\n",
    "                \n",
    "                # Predict activity\n",
    "                keypoints_input = keypoints.reshape(1, -1)\n",
    "                activity_probs = activity_model.predict(keypoints_input, verbose=0)[0]\n",
    "                predicted_class_idx = np.argmax(activity_probs)\n",
    "                predicted_class = label_encoder.classes_[predicted_class_idx]\n",
    "                confidence = activity_probs[predicted_class_idx]\n",
    "                \n",
    "                # Assess correctness\n",
    "                class_one_hot = to_categorical([predicted_class_idx], num_classes=len(label_encoder.classes_))\n",
    "                reconstructed = correctness_model.predict([keypoints_input, class_one_hot], verbose=0)\n",
    "                reconstruction_error = np.mean((keypoints_input - reconstructed) ** 2)\n",
    "                \n",
    "                if predicted_class in class_errors:\n",
    "                    threshold = class_errors[predicted_class]['threshold']\n",
    "                    correctness_score = max(0, min(100, 100 * (1 - reconstruction_error / threshold)))\n",
    "                else:\n",
    "                    correctness_score = 50\n",
    "                \n",
    "                # Draw pose\n",
    "                annotated_frame = draw_pose_on_image(frame, keypoints)\n",
    "                \n",
    "                # Add larger, more visible text\n",
    "                h, w, _ = annotated_frame.shape\n",
    "                \n",
    "                # Background rectangles for better text visibility\n",
    "                cv2.rectangle(annotated_frame, (5, 5), (400, 130), (0, 0, 0), -1)\n",
    "                \n",
    "                # Text overlays\n",
    "                cv2.putText(annotated_frame, f\"Activity: {predicted_class}\", (10, 35),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_frame, f\"Confidence: {confidence*100:.1f}%\", (10, 70),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                \n",
    "                # Color-coded form quality\n",
    "                form_color = (0, 255, 0) if correctness_score >= 70 else (0, 165, 255) if correctness_score >= 50 else (0, 0, 255)\n",
    "                cv2.putText(annotated_frame, f\"Form Quality: {correctness_score:.1f}%\", (10, 105),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, form_color, 2)\n",
    "                \n",
    "                # Show all class probabilities\n",
    "                y_offset = 160\n",
    "                cv2.rectangle(annotated_frame, (w - 305, 5), (w - 5, y_offset + len(label_encoder.classes_) * 35), (0, 0, 0), -1)\n",
    "                cv2.putText(annotated_frame, \"Probabilities:\", (w - 300, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "                for i, class_name in enumerate(label_encoder.classes_):\n",
    "                    prob = activity_probs[i]\n",
    "                    cv2.putText(annotated_frame, f\"{class_name}: {prob*100:.1f}%\", \n",
    "                               (w - 300, y_offset + i * 35),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                \n",
    "                frame = annotated_frame\n",
    "            else:\n",
    "                # No pose detected\n",
    "                cv2.putText(frame, \"No pose detected\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            # Display\n",
    "            cv2.imshow('AI Fitness Mirror - Live Feed (Press Q to quit)', frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Live feed stopped\")\n",
    "\n",
    "# Run live feed test\n",
    "if 'activity_model' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Ready to start live video feed test!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nUncomment the line below to start the webcam feed:\")\n",
    "    print(\"test_live_feed(activity_model, correctness_model, label_encoder, class_reconstruction_errors)\")\n",
    "    print(\"\\nNote: The webcam window will open. Press 'q' to quit.\")\n",
    "    \n",
    "    # Uncomment the next line to run the live feed\n",
    "    # test_live_feed(activity_model, correctness_model, label_encoder, class_reconstruction_errors)\n",
    "else:\n",
    "    print(\"Models not trained yet. Please train the models first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e6bfe0",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "**What this notebook does:**\n",
    "1. ✅ Extracts pose keypoints from images/videos using MediaPipe (COCO-compatible format)\n",
    "2. ✅ Generates skeletal visualizations saved to separate folders\n",
    "3. ✅ Saves all keypoints to a single JSON file for verification\n",
    "4. ✅ Trains a lightweight DNN to classify activities (pushups, situps, etc.)\n",
    "5. ✅ Trains a correctness model that learns from correct examples (no manual labeling needed)\n",
    "6. ✅ Tests on single images, videos, and live webcam feed\n",
    "\n",
    "**To get started:**\n",
    "1. Create a `training_data` folder with subfolders for each activity class:\n",
    "   ```\n",
    "   training_data/\n",
    "   ├── pushups/\n",
    "   │   ├── image1.jpg\n",
    "   │   └── video1.mp4\n",
    "   ├── situps/\n",
    "   │   ├── image1.jpg\n",
    "   │   └── video1.mp4\n",
    "   └── squats/\n",
    "       └── ...\n",
    "   ```\n",
    "2. Run all cells in order\n",
    "3. Check the `output/visualizations` folder for skeletal overlays\n",
    "4. Check `output/all_keypoints.json` for extracted data\n",
    "5. Test the models using the test cells\n",
    "\n",
    "**Key Features:**\n",
    "- **Lightweight**: Uses simple feedforward neural networks for fast inference\n",
    "- **No manual labeling for correctness**: The autoencoder learns what correct form looks like\n",
    "- **Real-time capable**: Can process live webcam feed\n",
    "- **Flexible**: Works with images, videos, and live streams"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
